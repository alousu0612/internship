{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "7OyCLf8b9EDL",
    "outputId": "f33497da-f041-443a-8b90-b67e4a32d012"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#NewsDataset class：將url中的Data轉成Bert input型態，包含tokens、masks、labels\n",
    "#tokens：可以理解為字元向量，因為有作padding，也就是讓各向量長度相同，補0之動作(padding主要是讓向量們可以使用矩陣運算，加快訓練速度)\n",
    "#masks ：用於區分是否為padding之element，tokens為1、paddings為0\n",
    "#labels：資料一開始的標籤，也就是用於訓練的答案\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, mode, url, tokenizer):\n",
    "        assert mode in [\"train\", \"predict\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(url, delimiter='\\t', header=None)\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "        \n",
    "#-------you might adjust codes here!!!!!!!------reset indices-----With different input data, codes here should be modified correspondingly\n",
    "        self.df = self.df.reset_index()\n",
    "        self.df = self.df.loc[:, [0, 1]]\n",
    "        self.df.columns = ['text', 'label']\n",
    "        self.text =self.df['text'].values\n",
    "        self.labels = self.df['label'].values\n",
    "        \n",
    "        #-------------convert to Bert-pretrained tokens-----------------\n",
    "        self.tokens = []\n",
    "        for sent in self.text:\n",
    "        # `encode` will:(1) Tokenize the sentence.(2) Prepend the `[CLS]` token to the start. (3) Append the `[SEP]` token to the end.(4) Map tokens to their IDs.\n",
    "           encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "           self.tokens.append(encoded_sent)\n",
    "        \n",
    "        #-------------padding-------------------------------------------\n",
    "        MAX_LEN = max(len(self.tokens[i]) for i in range(len(self.tokens)))\n",
    "        self.tokens = pad_sequences(self.tokens, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "        #-------------Create attention masks----------------------------\n",
    "        attention_masks = []\n",
    "        # Create a mask of 1s for each token followed by 0s for padding\n",
    "        for seq in self.tokens:\n",
    "           mask = [int(i>0) for i in seq]\n",
    "           attention_masks.append(mask) \n",
    "\n",
    "        #-------------Convert to tensors--------------------------------\n",
    "        self.tokens = torch.tensor(self.tokens)\n",
    "        self.masks = torch.tensor(attention_masks)\n",
    "        self.labels = torch.tensor(self.labels)       \n",
    "\n",
    "\n",
    "    #---------inherit from Dataset needing define __len__ and __getitem__ methods--------------\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "       if self.mode == \"predict\":\n",
    "            token = self.tokens[idx]\n",
    "            mask =self.masks[idx]\n",
    "            label_tensor = None\n",
    "       else:\n",
    "            token = self.tokens[idx]\n",
    "            mask =self.masks[idx]\n",
    "            label = self.labels[idx]\n",
    "       return (token, mask, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iEgaI5cXbRa0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "def data_split(data_set,test_size=0.1,batch_size=20):\n",
    "    #-------------------Use 90% for training and 10% for validation.\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(data_set.tokens, data_set.labels, random_state=2020, test_size=test_size)\n",
    "    #-------------------Do the same for the masks.\n",
    "    train_masks, validation_masks, _, _ = train_test_split(data_set.masks, data_set.labels, random_state=2020, test_size=test_size)\n",
    "\n",
    "    # Create the DataLoader for our training set.\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    # Create the DataLoader for our validation set.\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "    return train_dataloader, validation_dataloader\n",
    "    \n",
    "#data_split[0] is train_dataloader while data_split[1] is validation_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBmJx9pifsjo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def trainprocess(model, train_dataloader, validation_dataloader):\n",
    "    #---------------If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    #---------------If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # Tell pytorch to run this model on the GPU.\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "    # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "    # I believe the 'W' stands for 'Weight Decay fix\"\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "    # Number of training epochs (authors recommend between 2 and 4)\n",
    "    epochs = 4\n",
    "    # Total number of training steps is number of batches * number of epochs.\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "\n",
    "    # This training code is based on the `run_glue.py` script here:\n",
    "    # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "\n",
    "    # Set the seed value all over the place to make this reproducible.\n",
    "    # with the same seed value, one can produce same random list in every execution\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "    \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "    \n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('================== Epoch {:} / {:} =================='.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "\n",
    "        # Put the model into training mode. \n",
    "        # Don't be mislead--the call to \"train\" just changes the *mode*, it doesn't *perform* the training.\n",
    "        # \"dropout\" and \"batchnorm\" layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "    \n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches and show execution time for each step\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \"to\" method.\n",
    "            # batch contains three pytorch tensors:\n",
    "            #   [0]: input tokens \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].type(torch.LongTensor)\n",
    "            b_input_mask = batch[1].type(torch.LongTensor)\n",
    "            b_labels = batch[2].type(torch.LongTensor)\n",
    "        \n",
    "                    \n",
    "            b_input_ids =  b_input_ids.to(device)\n",
    "            b_input_mask = b_input_mask.to(device)\n",
    "            b_labels = b_labels.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            model.zero_grad()\n",
    "\n",
    "        \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we have provided the \"labels\".\n",
    "            # The documentation for this \"model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            \n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "               \n",
    "            # The call to \"model\" always returns a tuple, so we need to pull the loss value out of the tuple.\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end. \n",
    "            # \"loss\" is a Tensor containing a single value; the \".item()\" function just returns the Python value from the tensor.\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "        # Store the loss value for plotting the learning curve.\n",
    "        loss_values.append(avg_train_loss)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "        \n",
    "            # Add batch to GPU\n",
    "            #batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "            # Unpack the inputs from our dataloader\n",
    "            #b_input_ids, b_input_mask, b_labels = batch\n",
    "            b_input_ids = batch[0].type(torch.LongTensor)\n",
    "            b_input_mask = batch[1].type(torch.LongTensor)\n",
    "            b_labels = batch[2].type(torch.LongTensor)\n",
    "        \n",
    "        \n",
    "            b_input_ids =  b_input_ids.to(device)\n",
    "            b_input_mask = b_input_mask.to(device)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # This will return the logits rather than the loss because we have not provided labels.\n",
    "                # token_type_ids is the same as the \"segment ids\", which differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this \"model\" function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        \n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the outputvalues prior to applying an activation function like the softmax.\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "            # Calculate the accuracy for this batch of test sentences.\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "            # Accumulate the total accuracy.\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            # Track the number of batches\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    print(\"memory used:{:}\".format(torch.cuda.memory_allocated(device=0)))\n",
    "    print(\"cache used:{:}\".format(torch.cuda.memory_cached(device=0)))\n",
    "    del optimizer, scheduler\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print(\"memory used:{:}\".format(torch.cuda.memory_allocated(device=0)))\n",
    "    print(\"cache used:{:}\".format(torch.cuda.memory_cached(device=0)))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AN1I2-4Sva1t",
    "outputId": "d1b4a7c6-8b7b-45bb-b03b-bcce917c3f2c"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._C' has no attribute '_cuda_memoryStats'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6102f46422b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjupyter_client\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory used:{:}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_allocated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cache used:{:}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_cached\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\cuda\\memory.py\u001b[0m in \u001b[0;36mmemory_allocated\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0mdetails\u001b[0m \u001b[0mabout\u001b[0m \u001b[0mGPU\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0mmanagement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m     \"\"\"\n\u001b[1;32m--> 278\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmemory_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"allocated_bytes.all.current\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\cuda\\memory.py\u001b[0m in \u001b[0;36mmemory_stats\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m     \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory_stats_as_nested_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m     \u001b[0m_recurse_add_to_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\cuda\\memory.py\u001b[0m in \u001b[0;36mmemory_stats_as_nested_dict\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;34mr\"\"\"Returns the result of :func:`~torch.cuda.memory_stats` as a nested dictionary.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_memoryStats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_cuda_memoryStats'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc, os\n",
    "import jupyter_client\n",
    "print(\"memory used:{:}\".format(torch.cuda.memory_allocated(device=0)))\n",
    "print(\"cache used:{:}\".format(torch.cuda.memory_cached(device=0)))\n",
    "\n",
    "#del model\n",
    "torch.cuda.reset_max_memory_cached(device=0)\n",
    "\n",
    "torch.cuda.init()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.cuda.empty_cache()\n",
    "#a=torch.cuda.memory_stats(device=0)\n",
    "torch.cuda.reset_max_memory_allocated(device=0)\n",
    "\n",
    "jupyter_client.KernelManager.shutdown_kernel\n",
    "os._exit\n",
    "print(\"memory:{:}\".format(torch.cuda.memory_allocated(device=0)))\n",
    "print(\"cache:{:}\".format(torch.cuda.memory_cached(device=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoWYh19HGkHV"
   },
   "outputs": [],
   "source": [
    "def model_prediction(model, tokenizer, sentences):\n",
    "    #---------------If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU.    \n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    #---------------If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "   \n",
    "    encoded_token=[]\n",
    "    for sent in sentences:\n",
    "      encoded_sent = tokenizer.encode(sent,add_special_tokens = True)\n",
    "      encoded_token.append(encoded_sent)\n",
    " \n",
    "    MAX_LEN = max(len(sent) for sent in encoded_token)\n",
    "    encoded_id = pad_sequences(encoded_token, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    encoded_id=torch.tensor(encoded_id)  \n",
    "    \n",
    "      \n",
    "    model.eval()   \n",
    "    b_input_ids = encoded_id.type(torch.LongTensor)\n",
    "    b_input_ids = b_input_ids.to(device)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():             \n",
    "        outputs = model(b_input_ids)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        print(outputs[0])\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        result=np.argmax(logits, axis=1).flatten()\n",
    "        print(result)\n",
    "\n",
    "import os\n",
    "#Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "def save_model(model, tokenizer, output_dir):\n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(\"Saving model to %s\" % output_dir)\n",
    "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "    # They can then be reloaded using `from_pretrained()`\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCQaRe5hGkHa",
    "outputId": "dae55912-4cb6-48a2-e73e-6a10aff0e86f"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'C:/Users/vtteam/Documents/bert-base-uncased'. Make sure that:\n\n- 'C:/Users/vtteam/Documents/bert-base-uncased' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'C:/Users/vtteam/Documents/bert-base-uncased' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7adb84dc81f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m               \u001b[1;31m# The number of output labels--2 for binary classification and you can increase this for multi-class tasks.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0moutput_attentions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[1;31m# Whether the model returns attentions weights.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0moutput_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Whether the model returns all hidden-states.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m                 \u001b[0mlocal_files_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m             )\n\u001b[0;32m    605\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m                 \u001b[1;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             )\n\u001b[1;32m--> 251\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load config for 'C:/Users/vtteam/Documents/bert-base-uncased'. Make sure that:\n\n- 'C:/Users/vtteam/Documents/bert-base-uncased' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'C:/Users/vtteam/Documents/bert-base-uncased' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "model_version = 'C:/Users/vtteam/Documents/bert-base-uncased'   #此處可以更改訓練語言，中文pretrain model為'bert-base-chinese'\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_version,          # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2,               # The number of output labels--2 for binary classification and you can increase this for multi-class tasks.   \n",
    "    output_attentions = False,    # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version)\n",
    "\n",
    "#-------------------you can use either data url or it's path as input-----------------------------------------------\n",
    "data_set_url='https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv'\n",
    "#----you might need to adjust the codes in NewsDataSet class depending on which columns in CSV are training contents and labels\n",
    "data_set = NewsDataset(\"train\", data_set_url, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "#----split your data as training set and validation set-------------------------------------------------------------\n",
    "#----data_split(data_set,test_size=0.1,batch_size=20) as you can see test_size and batch_size have default values---\n",
    "dataloader = data_split(data_set)\n",
    "train_dataloader =dataloader[0]\n",
    "validation_dataloader=dataloader[1]\n",
    "\n",
    "\n",
    "#----train!!!!!------------------------------------------------------------------------------------------------------\n",
    "model=trainprocess(model,train_dataloader,validation_dataloader)\n",
    "\n",
    "\n",
    "#----use model to predict--------------------------------------------------------------------------------------------\n",
    "sentences =['South Korea’s Kospi led losses among the region’s major markets as it dropped 6.86% while the Kosdaq index fell 7.5%.',\n",
    "            'Stocks tumbled on Wednesday, reaching a new coronavirus crisis low as investors worried about the economic damage from the pandemic.',\n",
    "            'That is a terrible movie',\n",
    "            'Dow rebounds more than 1,000 points as Trump seeks $1 trillion in stimulus for coronavirus fight']\n",
    "model_prediction(model, tokenizer, sentences)\n",
    "\n",
    "\n",
    "#----save trained-model-----------------------------------------------------------------------------------------------\n",
    "output_dir = './model_save/'\n",
    "#save_model(model, tokenizer, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6RhZFmKGkHd"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKU8RUMRGkHh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
